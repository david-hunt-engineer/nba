{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import pyarrow\n",
    "import boto3\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import nba_py\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dropout, Dense, Input, LSTM, GRU, Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_scores_2000 = pd.read_parquet('boxscores_raw_2000-01.parquet')\n",
    "raw_scores_2001 = pd.read_parquet('boxscores_raw_2001-02.parquet')\n",
    "raw_scores_2002 = pd.read_parquet('boxscores_raw_2002-03.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_team_ids_dict():\n",
    "    '''\n",
    "    Get unique team ids dictionary for 1 hot encoding\n",
    "    '''\n",
    "    return {nba_py.constants.TEAMS[team]['id']: i for i, team in enumerate(nba_py.constants.TEAMS)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "class team_sequence_generator():\n",
    "    def __init__(self, input_df, sequence_length=20, process_on_init=False):\n",
    "        self.raw_df = input_df\n",
    "        self.df = None\n",
    "        self.team_dict = get_team_ids_dict()\n",
    "        self.sequence_data = None\n",
    "    \n",
    "        self.four_factor_cols = ['home_game', 'EFG_PCT', 'FTA_RATE', 'TM_TOV_PCT', 'OREB_PCT',\n",
    "                                 'OPP_EFG_PCT', 'OPP_FTA_RATE', 'OPP_TOV_PCT', 'OPP_OREB_PCT']\n",
    "\n",
    "        self.feature_cols = ['home_game', 'EFG_PCT', 'FTA_RATE', 'TM_TOV_PCT', 'OREB_PCT',\n",
    "                             'OPP_EFG_PCT', 'OPP_FTA_RATE', 'OPP_TOV_PCT', 'OPP_OREB_PCT',\n",
    "                             'OFF_RATING', 'DEF_RATING', 'NET_RATING', 'AST_PCT', 'AST_TOV',\n",
    "                             'AST_RATIO', 'DREB_PCT', 'REB_PCT', 'TS_PCT', 'USG_PCT', 'PACE', 'PIE',\n",
    "                             'PTS_OFF_TOV', 'PTS_2ND_CHANCE', 'PTS_FB', 'PTS_PAINT',\n",
    "                             'OPP_PTS_OFF_TOV', 'OPP_PTS_2ND_CHANCE', 'OPP_PTS_FB', 'OPP_PTS_PAINT',\n",
    "                             'BLK', 'BLKA', 'PF', 'PFD', 'PCT_FGA_2PT', 'PCT_FGA_3PT', 'PCT_PTS_2PT',\n",
    "                             'PCT_PTS_2PT_MR', 'PCT_PTS_3PT', 'PCT_PTS_FB', 'PCT_PTS_FT',\n",
    "                             'PCT_PTS_OFF_TOV', 'PCT_PTS_PAINT', 'PCT_AST_2PM', 'PCT_UAST_2PM',\n",
    "                             'PCT_AST_3PM', 'PCT_UAST_3PM', 'PCT_AST_FGM', 'PCT_UAST_FGM',\n",
    "                             'PTS_QTR1', 'PTS_QTR2', 'PTS_QTR3', 'PTS_QTR4',\n",
    "                             'LARGEST_LEAD', 'LEAD_CHANGES']\n",
    "\n",
    "        self.info_cols = ['GAME_ID', 'TEAM_ID', 'HOME_TEAM_ID', 'VISITOR_TEAM_ID', 'GAME_DATE_EST', 'PTS', 'ngame']\n",
    "        \n",
    "        if process_on_init:\n",
    "            self.tidy_raw_data()\n",
    "            self.generate_sequences(self.feature_cols, 20)\n",
    "\n",
    "    def get_team_ids_dict(self):\n",
    "        '''\n",
    "        Get unique team ids dictionary for 1 hot encoding\n",
    "        '''\n",
    "        return {nba_py.constants.TEAMS[team]['id']: i for i, team in enumerate(nba_py.constants.TEAMS)}\n",
    "    \n",
    "    \n",
    "    def normalise_input_features(self, input_df, features):\n",
    "        df = input_df.copy()\n",
    "\n",
    "        for feature in features:\n",
    "            feat_max = df[feature].describe()['max']\n",
    "            feat_mean = df[feature].describe()['mean']\n",
    "            feat_std = df[feature].describe()['std']\n",
    "\n",
    "            if feat_max > 1:\n",
    "                df[feature] = (df[feature] - feat_mean) / feat_std\n",
    "\n",
    "        return df\n",
    "\n",
    "    def tidy_raw_data(self):\n",
    "        # Munge a bit\n",
    "        df = self.raw_df.copy()\n",
    "        \n",
    "        \n",
    "        df['ngame'] = df[['GAME_ID']].apply(lambda x: int(x.GAME_ID[-4:]), axis=1)\n",
    "        df['home_game'] = (df.TEAM_ID == df.HOME_TEAM_ID).astype(int)\n",
    "        \n",
    "        df = df[self.feature_cols + self.info_cols]\n",
    "        \n",
    "        df.loc[:, 'GAME_DATE_EST'] = pd.to_datetime(df['GAME_DATE_EST'], format='%Y-%m-%d')\n",
    "        df = self.normalise_input_features(df, self.feature_cols)\n",
    "        \n",
    "        self.df = df\n",
    "        return df\n",
    "\n",
    "\n",
    "    def generate_sequences(self, features, sequence_length=15):\n",
    "        df = self.df.copy()\n",
    "        team_dict = get_team_ids_dict()\n",
    "\n",
    "        team_a_sequences = np.empty((0, sequence_length, len(features)))\n",
    "        team_b_sequences = np.empty((0, sequence_length, len(features)))\n",
    "        targets = np.empty(0)\n",
    "        home_inputs = np.empty(0)\n",
    "        team_a_index = np.empty(0)\n",
    "        team_b_index = np.empty(0)\n",
    "\n",
    "        df.TEAM_ID = df.TEAM_ID.astype(str)\n",
    "        df.VISITOR_TEAM_ID = df.VISITOR_TEAM_ID.astype(str)\n",
    "        df.HOME_TEAM_ID = df.HOME_TEAM_ID.astype(str)\n",
    "\n",
    "        for game in df.ngame:\n",
    "            home_team, away_team = df.loc[df.ngame == game, ['HOME_TEAM_ID', 'VISITOR_TEAM_ID']].values[0]\n",
    "\n",
    "            #Keep current game to get target, then remove\n",
    "            home_df = df.loc[(df.TEAM_ID == home_team) & (df.ngame <= game), :]\n",
    "            away_df = df.loc[(df.TEAM_ID == away_team) & (df.ngame <= game), :]\n",
    "\n",
    "            home_win = int(home_df.loc[home_df.ngame == game, 'PTS'].values[0] >\n",
    "                           away_df.loc[away_df.ngame == game, 'PTS'].values[0])\n",
    "\n",
    "            home_df = home_df.loc[df.ngame < game, :]\n",
    "            away_df = away_df.loc[df.ngame < game, :]\n",
    "\n",
    "            if home_df.shape[0] > sequence_length and away_df.shape[0] > sequence_length:\n",
    "                #TODO: DO SOME PADDING\n",
    "                #             home_sequence = np.zeros((sequence_length, four_factors.shape[0]))\n",
    "                home_sequence = home_df.iloc[:sequence_length, :][features].values\n",
    "                away_sequence = away_df.iloc[:sequence_length, :][features].values\n",
    "\n",
    "                # Add Home team as TEAM A\n",
    "                team_a_sequences = np.append(team_a_sequences, [home_sequence], axis=0)\n",
    "                team_a_index = np.append(team_a_index, self.team_dict[home_team])\n",
    "                team_b_sequences = np.append(team_b_sequences, [away_sequence], axis=0)\n",
    "                team_b_index = np.append(team_b_index, self.team_dict[away_team])\n",
    "                home_inputs = np.append(home_inputs, [1])\n",
    "\n",
    "                # Add Away team as TEAM A\n",
    "                team_a_sequences = np.append(team_a_sequences, [away_sequence], axis=0)\n",
    "                team_a_index = np.append(team_a_index, self.team_dict[away_team])\n",
    "                team_b_sequences = np.append(team_b_sequences, [home_sequence], axis=0)\n",
    "                team_b_index = np.append(team_b_index, self.team_dict[home_team])\n",
    "                home_inputs = np.append(home_inputs, [0])\n",
    "\n",
    "                targets = np.append(targets, [home_win] * 2)\n",
    "\n",
    "        self.sequence_data = {'team_a_sequences': team_a_sequences, 'team_b_sequences': team_b_sequences, \n",
    "                              'team_a_index': team_a_index, 'team_b_sequence':team_b_index, \n",
    "                              'home_inputs': home_inputs, 'results':targets}\n",
    "        return self.sequence_data \n",
    "\n",
    "\n",
    "y2000_sequences = team_sequence_generator(raw_scores_2000, process_on_init=True)\n",
    "y2001_sequences = team_sequence_generator(raw_scores_2001, process_on_init=True)\n",
    "y2002_sequences = team_sequence_generator(raw_scores_2002, process_on_init=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xa = np.concatenate((Xa_2000, Xa_2001, Xa_2002[:1000]))\n",
    "Xb = np.concatenate((Xb_2000, Xb_2001, Xb_2002[:1000]))\n",
    "teamida = np.concatenate((team_id_a_2000, team_id_a_2001, team_id_a_2002[:1000]))\n",
    "teamidb = np.concatenate((team_id_b_2000, team_id_b_2001, team_id_b_2002[:1000]))\n",
    "home_inputs = np.concatenate((home_inputs_2000, home_inputs_2001, home_inputs_2002[:1000]))\n",
    "y  = np.concatenate((y_2000, y_2001, y_2002[:1000]))\n",
    "\n",
    "Xa_test = Xa_2002[1000:]\n",
    "Xb_test = Xb_2002[1000:]\n",
    "teamida_test = team_id_a_2002[1000:]\n",
    "teamidb_test = team_id_b_2002[1000:]\n",
    "home_inputs_test = home_inputs_2002[1000:]\n",
    "y_test = y_2002[1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_comb = np.concatenate((X_2000, X_2001), axis=0)\n",
    "y_comb = np.concatenate((y_2000, y_2001), axis=0)\n",
    "home_inputs_comb = np.concatenate((home_inputs_2000, home_inputs_2001), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1920,)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.layers import Lambda\n",
    "# def build_model(X_train_a, X_train_b, team_id_train_a, team_id_train_b, home_input_train, y_train,\n",
    "#                 X_test_a, X_test_b, team_id_train_a, team_id_train_b, home_input_test, y_test):\n",
    "\n",
    "def initialise_model(sequence_length, feature_length, team_vector_dim):\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    hidden_dimension = team_vector_dim\n",
    "    \n",
    "    team_a_index = Input(shape=(1,), name='team_a_id_input')\n",
    "    team_b_index = Input(shape=(1,), name='team_b_id_input') \n",
    "    team_embedding = Embedding(len(get_team_ids_dict())+1, team_vector_dim)\n",
    "\n",
    "    squeeze = Lambda(lambda x: K.squeeze(x, axis=1))\n",
    "    team_a_encoded =  squeeze(team_embedding(team_a_index))\n",
    "    #print(team_embedding(team_a_index).shape)\n",
    "    team_b_encoded = squeeze(team_embedding(team_b_index))\n",
    "   \n",
    "    team_a = Input(shape=(sequence_length, feature_length))\n",
    "    team_b = Input(shape=(sequence_length, feature_length))\n",
    "    \n",
    "    shared_gru = GRU(hidden_dimension)\n",
    "    \n",
    "    # When we reuse the same layer instance multiple times, the weights of the layer\n",
    "    # are also being reused (it is effectively *the same* layer)\n",
    "    # Also: Initialise hidden state of the Payer level GRU with the player vector\n",
    "    print(team_a_encoded.shape)\n",
    "    encoded_a = shared_gru(team_a, initial_state=team_a_encoded)\n",
    "    encoded_b = shared_gru(team_b, initial_state=team_b_encoded)\n",
    "\n",
    "    # We can then concatenate the two vectors:\n",
    "    \n",
    "    gru_out = keras.layers.concatenate([encoded_a, encoded_b], axis=-1)\n",
    "#     gru_flat = Flatten()(gru_out)\n",
    "    \n",
    "    # Add aux features\n",
    "    # TODO: Add more aux. features, eg. TSL game, time, games in last week.\n",
    "    # TODO: ADD TEAM EMBEDDINGS\n",
    "    home_input = Input(shape=(1,), name='home_input')\n",
    "    merged_vector = keras.layers.concatenate([gru_out, home_input])\n",
    "\n",
    "    dense_pre_out = Dropout(0.25)(Dense(128)(merged_vector))\n",
    "    \n",
    "    # And add a logistic regression on top\n",
    "    predictions = Dense(1, activation='sigmoid')(dense_pre_out)\n",
    "\n",
    "    model = Model(inputs=[team_a, team_b, team_a_index, team_b_index, home_input], outputs=predictions)\n",
    "\n",
    "    model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_model(model, X_train_a, X_train_b, team_id_train_a, team_id_train_b, home_input_train, y_train,\n",
    "                X_test_a, X_test_b, team_id_test_a, team_id_test_b, home_input_test, y_test):\n",
    "    team_id_train_a = team_id_train_a.astype(int)\n",
    "    team_id_train_b = team_id_train_b.astype(int)\n",
    "    team_id_test_a = team_id_test_a.astype(int)\n",
    "    team_id_test_b = team_id_test_b.astype(int)\n",
    "    \n",
    "    thing = [X_train_a, X_train_b, team_id_train_a, team_id_train_b, home_input_train,\n",
    "            X_test_a, X_test_b, team_id_test_a, team_id_test_b, home_input_test, y_test]\n",
    "    for i in thing:\n",
    "        print(i.shape)\n",
    "    model.fit([X_train_a, X_train_b, team_id_train_a, team_id_train_b, home_input_train], \n",
    "              y_train, epochs=15, batch_size=2)   \n",
    "    \n",
    "    return model\n",
    "\n",
    "def eval_model(model, X_train_a, X_train_b, team_id_train_a, team_id_train_b, home_input_train, y_train,\n",
    "                X_test_a, X_test_b, team_id_test_a, team_id_test_b, home_input_test, y_test):\n",
    "#     print(model.summary())\n",
    "\n",
    "    scores_train = model.evaluate([X_train_a, X_train_b, team_id_train_a, team_id_train_b,\n",
    "                             home_input_train], y_train, verbose=0)\n",
    "    scores = model.evaluate([X_test_a, X_test_b, team_id_test_a, team_id_test_b,\n",
    "                             home_input_test], y_test, verbose=0)\n",
    "#     preds_train = model.predict_classes(X_test)\n",
    "\n",
    "    \n",
    "    print(\"Train Accuracy: %.2f%%\" % (scores_train[1]*100))\n",
    "    print(\"Test Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "    print(\"Train Loss: %.2f%%\" % (scores_train[0]))\n",
    "    print(\"Test Loss: %.2f%%\" % (scores[0]))\n",
    "    #     preds = model. ([X_test_a, X_test_b, home_input_test])\n",
    "    \n",
    "#     print(np.unique(preds_train))\n",
    "#     print(np.unique(preds))\n",
    "\n",
    "\n",
    "    \n",
    "model = initialise_model(Xa_2000.shape[1], Xa_2000.shape[2], 64)\n",
    "model = train_model(model, Xa, Xb, teamida, teamidb, home_inputs, y,\n",
    "                    Xa_test, Xb_test, teamida_test, teamidb_test, home_inputs_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 60.32%\n",
      "Test Accuracy: 62.71%\n",
      "Train Loss: 0.68%\n",
      "Test Loss: 0.68%\n"
     ]
    }
   ],
   "source": [
    "eval_model(model, Xa, Xb, teamida, teamidb, home_inputs, y,\n",
    "           Xa_test, Xb_test, teamida_test, teamidb_test, home_inputs_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_sequential_basic(X_train, y_train, X_test=None, y_test=None, train_ratio = 0.9):\n",
    "    numpy.random.seed(12)\n",
    "    \n",
    "    if X_test is None:    \n",
    "        train_ratio = 0.9\n",
    "        train_max_idx = int(train_ratio*X.shape[0])\n",
    "\n",
    "        X_test = X_train[train_max_idx:]\n",
    "        y_test = y[train_max_idx:]\n",
    "        \n",
    "        X_train = X_train[:train_max_idx]\n",
    "        y_train = y[:train_max_idx]\n",
    "        \n",
    "    model = Sequential()\n",
    "    model.add(GRU(100, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    model.add(Input(input_shape=(X_train.shape[0], 1)))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    model.fit(X_train, y_train, epochs=3, batch_size=32)\n",
    "    \n",
    "    # Final evaluation of the model\n",
    "    scores_train = model.evaluate(X_train, y_train, verbose=0)\n",
    "    scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "    preds_train = model.predict_classes(X_test)\n",
    "    preds = model.predict_classes(X_test)\n",
    "    \n",
    "    print(np.unique(preds_train))\n",
    "    print(np.unique(preds))\n",
    "    \n",
    "    print(\"Train Accuracy: %.2f%%\" % (scores_train[1]*100))\n",
    "    print(\"Test Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "train_sequential(X_2000, y_2000, X_2001, y_2001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(model.predict_classes(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 728, 1014], dtype=int64)"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.bincount(y_test.astype(int))#/y_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 5000)\n",
      "(250, 20, 2)\n",
      "(500, 20, 2)\n"
     ]
    }
   ],
   "source": [
    "def get_sin_cos_sequences(time_steps=20, count_rows=500, f1=1, f2=1.5, a1=1, a2=0.8):\n",
    "    \"\"\"\n",
    "    Generate some dummy data for testing sequence to sequence clustering\n",
    "    - Sine and cosine curves, with parameters provided as function inputs\n",
    "    \"\"\"\n",
    "    \n",
    "    x = np.arange(time_steps)\n",
    "    \n",
    "    _, X = np.mgrid[0:count_rows//2, 0:time_steps:1]\n",
    "    targets = np.zeros(count_rows)\n",
    "    targets[:count_rows//2] = 1\n",
    "    rands1 = f1 * 2 * np.pi * np.random.rand(count_rows//2, 1)\n",
    "    rands2 = f2 * 2 * np.pi * np.random.rand(count_rows//2, 1)\n",
    "    \n",
    "    feature_length = 2\n",
    "    \n",
    "    # sequences1a shape: 250, 50\n",
    "    sequences1a =   a1 * np.sin(f1*X*rands1/time_steps)\n",
    "    sequences1b =   np.random.rand(1) * a1 * np.sin(f1*X*rands1/time_steps)\n",
    "    \n",
    "    # Want sequences to be 250, 50, 2\n",
    "    sequences1 = np.concatenate(([np.ravel(sequences1a)], [np.ravel(sequences1b)]), axis=0) \n",
    "    print(sequences1.shape)\n",
    "    sequences1 = sequences1.T.reshape(count_rows//2, time_steps, feature_length)\n",
    "    print(sequences1.shape)\n",
    "\n",
    "    sequences2a =   a2 * np.cos(f2*X*rands2/time_steps)\n",
    "    sequences2b =   np.random.rand(1) * a2* np.cos(f2*X*rands2/time_steps)\n",
    "    sequences2 = np.concatenate(([np.ravel(sequences2a)], [np.ravel(sequences2b)]), axis=0) \n",
    "    sequences2 = sequences2.T.reshape(count_rows//2, time_steps, feature_length)\n",
    "    \n",
    "    sequences = np.concatenate((sequences1, sequences2), axis=0)\n",
    "    print(sequences.shape)\n",
    "    \n",
    "#     for i in range(count_rows//2):\n",
    "#         plt.plot(x, sequences1[i], alpha=0.05)\n",
    "#         plt.plot(x, sequences1b[i], alpha=0.05)\n",
    "#         plt.plot(x, sequences2[i], alpha=0.05)\n",
    "\n",
    "#     plt.xlabel('sample(n)')\n",
    "#     plt.ylabel('voltage(V)')\n",
    "#     plt.show()\n",
    "    \n",
    "#     sequences = np.vstack([sequences1, sequences2])\n",
    "    return sequences, targets #sequences.reshape(count_rows, time_steps, 1)\n",
    "#     \n",
    "X_dummy, y_dummy = get_sin_cos_sequences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1754, 20, 5)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_26 (GRU)                 (None, 100)               30900     \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 31,001\n",
      "Trainable params: 31,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 0.6399 - acc: 0.6300\n",
      "Epoch 2/3\n",
      "500/500 [==============================] - 0s 566us/step - loss: 0.2560 - acc: 0.9240\n",
      "Epoch 3/3\n",
      "500/500 [==============================] - 0s 586us/step - loss: 0.0571 - acc: 0.9840\n",
      "Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "numpy.random.seed(12)\n",
    "\n",
    "train_ratio = 0.9\n",
    "train_max_idx = int(train_ratio*X.shape[0])\n",
    "\n",
    "# X_train = X[:train_max_idx]\n",
    "# y_train = y[:train_max_idx]\n",
    "# X_test = X[train_max_idx:]\n",
    "# y_test = y[train_max_idx:]\n",
    "print(X.shape)\n",
    "\n",
    "def train_sequential_dummy(X, y):\n",
    "    model = Sequential()\n",
    "    model.add(GRU(100, input_shape=(X.shape[1],X.shape[2])))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    model.fit(X, y, epochs=3, batch_size=32)\n",
    "    # Final evaluation of the model\n",
    "    scores = model.evaluate(X, y, verbose=0)\n",
    "    preds = model.predict(X)\n",
    "    print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "    \n",
    "train_sequential_dummy(X_dummy, y_dummy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
